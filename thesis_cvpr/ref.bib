@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@inproceedings{potion,
	author = {Choutas, Vasileios and Weinzaepfel, Philippe and Revaud, Jerome and Schmid, Cordelia},
	year = {2018},
	month = {06},
	pages = {7024-7033},
	title = {PoTion: Pose MoTion Representation for Action Recognition},
	doi = {10.1109/CVPR.2018.00734}
}

@INPROCEEDINGS{PA3D,
	author={Yan, An and Wang, Yali and Li, Zhifeng and Qiao, Yu},
	booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={PA3D: Pose-Action 3D Machine for Video Recognition}, 
	year={2019},
	volume={},
	number={},
	pages={7914-7923},
	doi={10.1109/CVPR.2019.00811}}

@INPROCEEDINGS{simple_yet_efficient,
	author={Ludl, Dennis and Gulde, Thomas and Curio, Cristóbal},
	booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
	title={Simple yet efficient real-time pose-based action recognition},
	year={2019},
	volume={},
	number={},
	pages={581-588},
	doi={10.1109/ITSC.2019.8917128}
}

@inproceedings{smaller_faster_better,
	author = {Yang, Fan and Wu, Yang and Sakti, Sakriani and Nakamura, Satoshi},
	title = {Make Skeleton-Based Action Recognition Model Smaller, Faster and Better},
	year = {2020},
	isbn = {9781450368414},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3338533.3366569},
	doi = {10.1145/3338533.3366569},
	abstract = {Although skeleton-based action recognition has achieved great success in recent years, most of the existing methods may suffer from a large model size and slow execution speed. To alleviate this issue, we analyze skeleton sequence properties to propose a Double-feature Double-motion Network (DD-Net) for skeleton-based action recognition. By using a lightweight network structure (i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500 FPS on an ordinary GPU (e.g., GTX 1080Ti), or, 2,000 FPS on an ordinary CPU (e.g., Intel E5-2620). By employing robust features, DD-Net achieves state-of-the-art performance on our experiment datasets: SHREC (i.e., hand actions) and JHMDB (i.e., body actions). Our code is on https://github.com/fandulu/DD-Net.},
	booktitle = {Proceedings of the ACM Multimedia Asia},
	articleno = {31},
	numpages = {6},
	keywords = {Skeleton-based Action Recognition, Body Actions, Hand Gestures},
	location = {Beijing, China},
	series = {MMAsia '19}
}

@ARTICLE{two_branch_stacked_lstm,
	author={Avola, Danilo and Cascio, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano and Rodolà, Emanuele},
	journal={IEEE Transactions on Multimedia}, 
	title={2-D Skeleton-Based Action Recognition via Two-Branch Stacked LSTM-RNNs}, 
	year={2020},
	volume={22},
	number={10},
	pages={2481-2496},
	doi={10.1109/TMM.2019.2960588}
}

@INPROCEEDINGS{RNN_joint_relative_motion,
	author={Wei, Shenghua and Song, Yonghong and Zhang, Yuanlin},
	booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
	title={Human skeleton tree recurrent neural network with joint relative motion feature for skeleton based action recognition}, 
	year={2017},
	volume={},
	number={},
	pages={91-95},
	doi={10.1109/ICIP.2017.8296249}
}

@ARTICLE{RNN_occlusion,
	author={Angelini, Federico and Fu, Zeyu and Long, Yang and Shao, Ling and Naqvi, Syed Mohsen},
	journal={IEEE Transactions on Multimedia}, 
	title={2D Pose-Based Real-Time Human Action Recognition With Occlusion-Handling}, 
	year={2020},
	volume={22},
	number={6},
	pages={1433-1446},
	doi={10.1109/TMM.2019.2944745}
}

@ARTICLE{DS_lstm,
	author={Jiang, Xinghao and Xu, Ke and Sun, Tanfeng},
	journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
	title={Action Recognition Scheme Based on Skeleton Representation With DS-LSTM Network}, 
	year={2020},
	volume={30},
	number={7},
	pages={2129-2140},	
	doi={10.1109/TCSVT.2019.2914137}
}

@article{depthcamera3dpose,
	title={A data-driven approach for real-time full body pose reconstruction from a depth camera},
	author={Baak, Andreas and M{\"u}ller, Meinard and Bharaj, Gaurav and Seidel, Hans-Peter and Theobalt, Christian},
	journal={Consumer Depth Cameras for Computer Vision: Research Topics and Applications},
	pages={71--98},
	year={2013},
	publisher={Springer}
}

@inproceedings{3dposefrom2d,
	title={3d human pose estimation= 2d pose estimation+ matching},
	author={Chen, Ching-Hang and Ramanan, Deva},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={7035--7043},
	year={2017}
}

@inproceedings{2dposefromrgb,
	title={Lifting from the deep: Convolutional 3d pose estimation from a single image},
	author={Tome, Denis and Russell, Chris and Agapito, Lourdes},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2500--2509},
	year={2017}
}

@inproceedings{3dposeactionrecognition,
	title={2d/3d pose estimation and action recognition using multitask deep learning},
	author={Luvizon, Diogo C and Picard, David and Tabia, Hedi},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={5137--5146},
	year={2018}
}
