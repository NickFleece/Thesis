% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Action Recognition Thesis - WIP}

\author{Nicolas Fleece\\
University of Ottawa\\
75 Laurier Ave. E, Ottawa, ON K1N 6N5\\
{\tt\small nflee092@uottawa.ca}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
   Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
   The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

CNN's have been widely used for many years on the problem of human action recognition. They have shown to be capable of detecting actions performed by people in simple and complex settings. This quite often involves the use of 3D-Convolutions, which operate on multiple frames of video at a time. Complex models that often use several GPUs are constantly being developed and tested on many different datasets.

\subsection{Pose-Based Action Recognition}

Pose involves extracting the skeleton of the person and using this data over multiple frames of a video to classify an action. Pose is a common addition used in action recognition as it relates most to how humans view actions and the movement of different bones.

\subsubsection{Intermediate Representations}

The approach of the majority of this paper involves creating intermediate representations for pose data over multiple frames. This typically has the aim of creating some kind of image that represents either the motion of the persons bones and/or joints through the image at different points in the video. These images can then be used either by the model independently or added to traditional two-stream architectures.

The advantage of these types of representations is that the model can be a small neural network (often CNN's) that can be trained end-to-end very quickly and with little memory. This quite often allows for real-time evaluation and in some cases mobile-capable models.

\subsection{Person-Based Action Recognition}

\section{Related Work}
\label{sec:relatedwork}

\textbf{Video Action Recognition.} 

% Potion
\textbf{Pose motion representation for action recognition.} \cite{potion} was largely the inspiration for the work that was done within this paper. This approach aims to take the joints extracted from the pose representation and use the movement over $f$ frames, creating $j$ images where $j$ is the number of joints.

The approach begins by extracting $j$ joint heat maps from each frame of the video, these individual frames are then combines using their colour coding where depending on what time $t$ the frame is at in the video, the joint heatmap is made to be that colour. They then perform their temporal aggregation where for each joint $j$, they combine all frames together into one image, perfoming a simple addition through all frames. This leaves an image that demonstrates the movement of one joint through all frames of a video.

% pa3d
\textbf{Pose action 3D.} \cite{PA3D} is a similar approach to PoTion, where it involves the use of the generated joint heat maps from pose estimation models. The difference is that instead of using the colour coding similar to potion, PA3D stacks the joint heat maps such that they create $j$ cubes of every heat map frame.

% simple one
\textbf{Simple yet efficient real-time pose-based action recognition.} \cite{simple_yet_efficient} is the most similar to our work, as it uses a matrix of x and y joint positions. A simple 2D-CNN is then run on this intermediate representation, which leads to a very lightweight model that provides accurate results on their datasets.

Other works have examined the idea of intermediate representations REFERENCES, some utilizing 3d pose REFERENCES. While the 3d pose may provide a potential improvement in performance, the current 3d-pose estimation models are not quite mature enough and our intermediate representation lends itself to 2d-pose more than 3d-pose. This may be a 

\section{Method}
\label{sec:method}

\section{Experiment}
\label{sec:experiment}

\section{Conclusion}
\label{sec:conclusion}

\section{Acknowledgement}
\label{sec:acknowledgement}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\end{document}
