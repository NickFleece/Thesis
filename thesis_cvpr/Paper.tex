% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Action Recognition Thesis - WIP}

\author{Nicolas Fleece\\
University of Ottawa\\
75 Laurier Ave. E, Ottawa, ON K1N 6N5\\
{\tt\small nflee092@uottawa.ca}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
   Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
   The abstract is to be in 10-point, single-spaced type.
   Leave two blank lines after the Abstract, then begin the main text.
   Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Human action recognition is a very difficult task and is a constantly evolving topic of research in the computer vision community. In the real world, human action recognition has many issues, not limited to things such as occlusion, cluttered and dynamic backgrounds, camera motion, and multiview variations \cite{recognitionchallenges}. Existing classical models \cite{i3d} are vulnerable to these variables in the real world. The extraction of different features has tried to mitigate these background influences, such as rgb flow \cite{rgbflow}, however these can still be influenced by motion in the background. The emergence of lightweight, high quality pose estimation \cite{openpose} has allowed for accurate skeleton data from nearly every video in the wild, and utilizing this skeleton data, the background factors can be mitigated.

This paper aims to mitigate these background factors, while providing a model that provides high-quality action recognition prediction at near state of the art performance.  This involves constructing an intermediate representation using exclusively 2D pose data. With this representation, we aim to remove as much background influence as possible, thus our representation is independent of global position, or global rotation of the subject performing the action. The architecture of this representation was greatly inspired by \cite{simple_yet_efficient}, however their implementation suffers from largely relying on the global position of the person, which can reduce in the wild accuracy outside of curated datasets.

\section{Related Work}
\label{sec:relatedwork}

 Typical examples of action recognition \cite{i3d} have used complex convolutional neural networks (CNNs) on the RGB frames, pulling features such as rgb flow \cite{rgbflow} in order to enhance results. This method has proved to work well, however these models often require large amounts of GPU memory and are required to be run on high end hardware, which is a potential issue when applied to real-world scenarios where the necessary hardware may not be available. In addition, background noise is much more difficult to filter out and generalizing to different environments is difficult.
 
 A subset of action recognition models utilize skeleton-based action recognition. These models aim to utilize the positions of joints/bones in the model in order to filter out background data, and allow the model to focus on the actual person rather than background data. Typically these involve simpler CNNs that allow for faster computation on lower quality hardware. Many different approaches are used to achieve this action recognition. \cite{potion}, \cite{PA3D} utilize processed joint heatmap images and simple 2D-CNNs. \cite{simple_yet_efficient} \cite{smaller_faster_better} use intermediate representations to construct custom image representations that can be easily processed by simple CNNs. RNNs and LSTMs \cite{two_branch_stacked_lstm} \cite{RNN_joint_relative_motion} \cite{RNN_occlusion} \cite{DS_lstm} and Transformers \cite{transformersnippets} \cite{transformertwobranch} have been used as well to obtain good results with skeleton data.

Almost always, these skeleton based models utilize 2D pose data. There have been some examples of extracting 3D pose from depth cameras \cite{depthcamera3dpose} as well as estimating 3D pose from 2D pose \cite{3dposefrom2d} and only the RGB frames \cite{2dposefromrgb}. These techniques have been used in the past for human action recognition \cite{3dposeactionrecognition}, however 2D pose estimation is a much easier task and the models that have been used are generally much higher quality, and therefore is more reliable for the task of human action recognition.

\section{Method}
\label{sec:method}

\section{Experiment}
\label{sec:experiment}

\section{Conclusion}
\label{sec:conclusion}

\section{Acknowledgement}
\label{sec:acknowledgement}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\end{document}
