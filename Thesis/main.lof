\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }}{7}{figure.caption.33}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Action recognition structure for the LRCN model. \blx@tocontentsinit {0}\cite {LRCNS}\relax }}{8}{figure.caption.34}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \blx@tocontentsinit {0}\cite {beyondshortsnippets}\relax }}{9}{figure.caption.35}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Deep LSTM architecture utilized by \blx@tocontentsinit {0}\cite {beyondshortsnippets} in the feature aggregation step as shown in figure \ref {fig:beyondoverview}.\relax }}{9}{figure.caption.36}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces The original 3D-CNN action recognition model architecture proposed by \blx@tocontentsinit {0}\cite {3DCNN-ActionRecognition}, containing 3 convolutional layers, two subsampling layers, and one fully connected layer\relax }}{10}{figure.caption.38}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces The model architecture used in the I3D paper \blx@tocontentsinit {0}\cite {i3d}, where the Inflated Inception-V1 architecture (left) and it's detailed submodule (right) are shown.\relax }}{11}{figure.caption.39}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces The original transformer model proposed in \blx@tocontentsinit {0}\cite {transformer_og}, the image is split into fixed-size patches, linearly embed them, and add positional embeddings. It is then fed into a standard Transformer Encoder architecture.\relax }}{11}{figure.caption.41}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \blx@tocontentsinit {0}\cite {transformer_og}\relax }}{12}{figure.caption.42}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces The illustration of the PoTion representation \blx@tocontentsinit {0}\cite {potion}. The input joint heatmaps are colored based on their time in the frame, and the frames are then concatenated to form the final movement of the joint throughout the video.\relax }}{14}{figure.caption.50}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces The colourization method utilized by the PoTion model \blx@tocontentsinit {0}\cite {potion}. As the frame index moves throughout the video, the colour of the joint shifts from one to another. This can be done for any amount of colours, denoted by C, the figure shows examples for C=2 and C=3, but the same logic holds for more than 3.\relax }}{15}{figure.caption.51}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces \relax }}{15}{figure.caption.52}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces \relax }}{16}{figure.caption.53}%
\addvspace {10\p@ }
