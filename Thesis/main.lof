\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }}{7}{figure.caption.33}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Action recognition structure for the LRCN model. \blx@tocontentsinit {0}\cite {LRCNS}\relax }}{8}{figure.caption.34}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \blx@tocontentsinit {0}\cite {beyondshortsnippets}\relax }}{9}{figure.caption.35}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Deep LSTM architecture utilized by \blx@tocontentsinit {0}\cite {beyondshortsnippets} in the feature aggregation step as shown in figure \ref {fig:beyondoverview}.\relax }}{9}{figure.caption.36}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces The original 3D-CNN action recognition model architecture proposed by \blx@tocontentsinit {0}\cite {3DCNN-ActionRecognition}, containing 3 convolutional layers, two subsampling layers, and one fully connected layer\relax }}{10}{figure.caption.38}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces The original transformer model proposed in \blx@tocontentsinit {0}\cite {transformer_og}, the image is split into fixed-size patches, linearly embed them, and add positional embeddings. It is then fed into a standard Transformer Encoder architecture.\relax }}{11}{figure.caption.40}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \blx@tocontentsinit {0}\cite {transformer_og}\relax }}{12}{figure.caption.41}%
\addvspace {10\p@ }
