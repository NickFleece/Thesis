@inproceedings{potion,
	author = {Choutas, Vasileios and Weinzaepfel, Philippe and Revaud, Jerome and Schmid, Cordelia},
	year = {2018},
	month = {06},
	pages = {7024-7033},
	title = {PoTion: Pose MoTion Representation for Action Recognition},
	doi = {10.1109/CVPR.2018.00734}
}

@INPROCEEDINGS{PA3D,
	author={Yan, An and Wang, Yali and Li, Zhifeng and Qiao, Yu},
	booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={PA3D: Pose-Action 3D Machine for Video Recognition}, 
	year={2019},
	volume={},
	number={},
	pages={7914-7923},
	doi={10.1109/CVPR.2019.00811}}

@INPROCEEDINGS{simple_yet_efficient,
	author={Ludl, Dennis and Gulde, Thomas and Curio, Cristóbal},
	booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
	title={Simple yet efficient real-time pose-based action recognition},
	year={2019},
	volume={},
	number={},
	pages={581-588},
	doi={10.1109/ITSC.2019.8917128}
}

@inproceedings{smaller_faster_better,
	author = {Yang, Fan and Wu, Yang and Sakti, Sakriani and Nakamura, Satoshi},
	title = {Make Skeleton-Based Action Recognition Model Smaller, Faster and Better},
	year = {2020},
	isbn = {9781450368414},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3338533.3366569},
	doi = {10.1145/3338533.3366569},
	abstract = {Although skeleton-based action recognition has achieved great success in recent years, most of the existing methods may suffer from a large model size and slow execution speed. To alleviate this issue, we analyze skeleton sequence properties to propose a Double-feature Double-motion Network (DD-Net) for skeleton-based action recognition. By using a lightweight network structure (i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500 FPS on an ordinary GPU (e.g., GTX 1080Ti), or, 2,000 FPS on an ordinary CPU (e.g., Intel E5-2620). By employing robust features, DD-Net achieves state-of-the-art performance on our experiment datasets: SHREC (i.e., hand actions) and JHMDB (i.e., body actions). Our code is on https://github.com/fandulu/DD-Net.},
	booktitle = {Proceedings of the ACM Multimedia Asia},
	articleno = {31},
	numpages = {6},
	keywords = {Skeleton-based Action Recognition, Body Actions, Hand Gestures},
	location = {Beijing, China},
	series = {MMAsia '19}
}

@ARTICLE{two_branch_stacked_lstm,
	author={Avola, Danilo and Cascio, Marco and Cinque, Luigi and Foresti, Gian Luca and Massaroni, Cristiano and Rodolà, Emanuele},
	journal={IEEE Transactions on Multimedia}, 
	title={2-D Skeleton-Based Action Recognition via Two-Branch Stacked LSTM-RNNs}, 
	year={2020},
	volume={22},
	number={10},
	pages={2481-2496},
	doi={10.1109/TMM.2019.2960588}
}

@INPROCEEDINGS{RNN_joint_relative_motion,
	author={Wei, Shenghua and Song, Yonghong and Zhang, Yuanlin},
	booktitle={2017 IEEE International Conference on Image Processing (ICIP)}, 
	title={Human skeleton tree recurrent neural network with joint relative motion feature for skeleton based action recognition}, 
	year={2017},
	volume={},
	number={},
	pages={91-95},
	doi={10.1109/ICIP.2017.8296249}
}

@ARTICLE{RNN_occlusion,
	author={Angelini, Federico and Fu, Zeyu and Long, Yang and Shao, Ling and Naqvi, Syed Mohsen},
	journal={IEEE Transactions on Multimedia}, 
	title={2D Pose-Based Real-Time Human Action Recognition With Occlusion-Handling}, 
	year={2020},
	volume={22},
	number={6},
	pages={1433-1446},
	doi={10.1109/TMM.2019.2944745}
}

@ARTICLE{DS_lstm,
	author={Jiang, Xinghao and Xu, Ke and Sun, Tanfeng},
	journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
	title={Action Recognition Scheme Based on Skeleton Representation With DS-LSTM Network}, 
	year={2020},
	volume={30},
	number={7},
	pages={2129-2140},	
	doi={10.1109/TCSVT.2019.2914137}
}

@article{depthcamera3dpose,
	title={A data-driven approach for real-time full body pose reconstruction from a depth camera},
	author={Baak, Andreas and M{\"u}ller, Meinard and Bharaj, Gaurav and Seidel, Hans-Peter and Theobalt, Christian},
	journal={Consumer Depth Cameras for Computer Vision: Research Topics and Applications},
	pages={71--98},
	year={2013},
	publisher={Springer}
}

@inproceedings{3dposefrom2d,
	title={3d human pose estimation= 2d pose estimation+ matching},
	author={Chen, Ching-Hang and Ramanan, Deva},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={7035--7043},
	year={2017}
}

@inproceedings{2dposefromrgb,
	title={Lifting from the deep: Convolutional 3d pose estimation from a single image},
	author={Tome, Denis and Russell, Chris and Agapito, Lourdes},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={2500--2509},
	year={2017}
}

@inproceedings{3dposeactionrecognition,
	title={2d/3d pose estimation and action recognition using multitask deep learning},
	author={Luvizon, Diogo C and Picard, David and Tabia, Hedi},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={5137--5146},
	year={2018}
}

@INPROCEEDINGS{transformersnippets,
	author={Askar, Aizada and Lee, Min-Ho and Huynh-The, Thien and Tu, Nguyen Anh},
	booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
	title={2D Skeleton-based Action Recognition Using Action-Snippets and Sequential Deep Learning}, 
	year={2022},
	volume={},
	number={},
	pages={2372-2377},	
	doi={10.1109/SMC53654.2022.9945402}}

@inproceedings{transformertwobranch,
	title={Two-Branch Stacked Transformer for 2D Skeleton-based Action Recognition},
	author={Zhalgasbayev, Yerassyl and Tu, Nguyen Anh},
	booktitle={2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)},
	pages={1--4},
	year={2023},
	organization={IEEE}
}

@inproceedings{i3d,
	title={Quo vadis, action recognition? a new model and the kinetics dataset},
	author={Carreira, Joao and Zisserman, Andrew},
	booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={6299--6308},
	year={2017}
}

@inproceedings{rgbflow,
	title={Representation flow for action recognition},
	author={Piergiovanni, AJ and Ryoo, Michael S},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={9945--9953},
	year={2019}
}

@article{recognitionchallenges,
	title = {Vision-based human action recognition: An overview and real world challenges},
	journal = {Forensic Science International: Digital Investigation},
	volume = {32},
	pages = {200901},
	year = {2020},
	issn = {2666-2817},
	doi = {https://doi.org/10.1016/j.fsidi.2019.200901},
	url = {https://www.sciencedirect.com/science/article/pii/S174228761930283X},
	author = {Imen Jegham and Anouar {Ben Khalifa} and Ihsen Alouani and Mohamed Ali Mahjoub},
	keywords = {Vision-based, Action recognition, Activity recognition, Real world challenges, Datasets},
	abstract = {Within a large range of applications in computer vision, Human Action Recognition has become one of the most attractive research fields. Ambiguities in recognizing actions does not only come from the difficulty to define the motion of body parts, but also from many other challenges related to real world problems such as camera motion, dynamic background, and bad weather conditions. There has been little research work in the real world conditions of human action recognition systems, which encourages us to seriously search in this application domain. Although a plethora of robust approaches have been introduced in the literature, they are still insufficient to fully cover the challenges. To quantitatively and qualitatively compare the performance of these methods, public datasets that present various actions under several conditions and constraints are recorded. In this paper, we investigate an overview of the existing methods according to the kind of issue they address. Moreover, we present a comparison of the existing datasets introduced for the human action recognition field.}
}

@inproceedings{openpose,
	title={Realtime multi-person 2d pose estimation using part affinity fields},
	author={Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={7291--7299},
	year={2017}
}


@ARTICLE{LRCNS,
	author={Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Long-Term Recurrent Convolutional Networks for Visual Recognition and Description}, 
	year={2017},
	volume={39},
	number={4},
	pages={677-691},
	doi={10.1109/TPAMI.2016.2599174}
}

@misc{LSTM-2015,
	title={Learning to Execute}, 
	author={Wojciech Zaremba and Ilya Sutskever},
	year={2015},
	eprint={1410.4615},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@article{caffenet,
	author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang},
	year = {2018},
	month = {01},
	pages = {149-180},
	title = {How convolutional neural networks see the world --- A survey of convolutional neural network visualization methods},
	volume = {1},
	journal = {Mathematical Foundations of Computing},
	doi = {10.3934/mfc.2018008}
}

@inproceedings{alexnet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	volume = {25},
	year = {2012}
}

@misc{zeilerfergus,
	title={Visualizing and Understanding Convolutional Networks}, 
	author={Matthew D Zeiler and Rob Fergus},
	year={2013},
	eprint={1311.2901},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@INPROCEEDINGS{beyondshortsnippets,
	author={Joe Yue-Hei Ng and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
	booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={Beyond short snippets: Deep networks for video classification}, 
	year={2015},
	volume={},
	number={},
	pages={4694-4702},
	doi={10.1109/CVPR.2015.7299101}
}

 @InProceedings{googlenet,
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	title = {Going Deeper With Convolutions},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2015}
}

@ARTICLE{3DCNN-ActionRecognition,
	author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={3D Convolutional Neural Networks for Human Action Recognition}, 
	year={2013},
	volume={35},
	number={1},
	pages={221-231},
	doi={10.1109/TPAMI.2012.59}}
	
@misc{Eldermonitoring,
	title={Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition}, 
	author={Han Sun and Yu Chen},
	year={2022},
	eprint={2207.10519},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{3x33dcnn,
	title={Learning Spatiotemporal Features with 3D Convolutional Networks}, 
	author={Du Tran and Lubomir Bourdev and Rob Fergus and Lorenzo Torresani and Manohar Paluri},
	year={2015},
	eprint={1412.0767},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{transformer_og,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year={2021},
	eprint={2010.11929},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@InProceedings{multiview_transformers,
	author    = {Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
	title     = {Multiview Transformers for Video Recognition},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2022},
	pages     = {3333-3343}
}

@INPROCEEDINGS{WangPose,
	author={Wang, Chunyu and Wang, Yizhou and Yuille, Alan L.},
	booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
	title={An Approach to Pose-Based Action Recognition}, 
	year={2013},
	volume={},
	number={},
	pages={915-922},	
	doi={10.1109/CVPR.2013.123}}

@INPROCEEDINGS{PCNN,
	author={Chéron, Guilhem and Laptev, Ivan and Schmid, Cordelia},
	booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
	title={P-CNN: Pose-Based CNN Features for Action Recognition}, 
	year={2015},
	volume={},
	number={},
	pages={3218-3226},
	doi={10.1109/ICCV.2015.368}}
	
@InProceedings{Chained,
	author = {Zolfaghari, Mohammadreza and Oliveira, Gabriel L. and Sedaghat, Nima and Brox, Thomas},
	title = {Chained Multi-Stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
	month = {Oct},
	year = {2017}
} 

@InProceedings{poseandjointaware,
	author    = {Shah, Anshul and Mishra, Shlok and Bansal, Ankan and Chen, Jun-Cheng and Chellappa, Rama and Shrivastava, Abhinav},
	title     = {Pose and Joint-Aware Action Recognition},
	booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	month     = {January},
	year      = {2022},
	pages     = {3850-3860}
}

@inproceedings{JHMDB,
	title = {Towards understanding action recognition},
	author = {H. Jhuang and J. Gall and S. Zuffi and C. Schmid and M. J. Black},
	booktitle = {International Conf. on Computer Vision (ICCV)},
	month = Dec,
	pages = {3192-3199},
	year = {2013}
}

@misc{vitpose,
	title={ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation}, 
	author={Yufei Xu and Jing Zhang and Qiming Zhang and Dacheng Tao},
	year={2022},
	eprint={2204.12484},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={https://www.tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dandelion~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@article{cifar,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Toronto, ON, Canada}
}

@misc{efficientnet,
	title={EfficientNetV2: Smaller Models and Faster Training}, 
	author={Mingxing Tan and Quoc V. Le},
	year={2021},
	eprint={2104.00298},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@InProceedings{TV-L,
	author="Zach, C.
	and Pock, T.
	and Bischof, H.",
	editor="Hamprecht, Fred A.
	and Schn{\"o}rr, Christoph
	and J{\"a}hne, Bernd",
	title="A Duality Based Approach for Realtime TV-L1 Optical Flow",
	booktitle="Pattern Recognition",
	year="2007",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="214--223",
	abstract="Variational methods are among the most successful approaches to calculate the optical flow between two image frames. A particularly appealing formulation is based on total variation (TV) regularization and the robust L1 norm in the data fidelity term. This formulation can preserve discontinuities in the flow field and offers an increased robustness against illumination changes, occlusions and noise. In this work we present a novel approach to solve the TV-L1 formulation. Our method results in a very efficient numerical scheme, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step. Additionally, our approach can be accelerated by modern graphics processing units. We demonstrate the real-time performance (30 fps) of our approach for video inputs at a resolution of 320{\texttimes}240 pixels.",
	isbn="978-3-540-74936-3"
}

@misc{charades,
	title={Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding}, 
	author={Gunnar A. Sigurdsson and Gül Varol and Xiaolong Wang and Ali Farhadi and Ivan Laptev and Abhinav Gupta},
	year={2016},
	eprint={1604.01753},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{THUMOS15,
	author = "Gorban, A. and Idrees, H. and Jiang, Y.-G. and  Roshan Zamir, A. and Laptev,
	I. and Shah, M. and Sukthankar, R.",
	title = "{THUMOS} Challenge: Action Recognition with a Large Number of Classes",
	howpublished = "\url{http://www.thumos.info/}",
	Year = {2015}}

@misc{kinetics,
	title={The Kinetics Human Action Video Dataset}, 
	author={Will Kay and Joao Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman},
	year={2017},
	eprint={1705.06950},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{ucf101,
	title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild}, 
	author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
	year={2012},
	eprint={1212.0402},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{kinetics700,
	title={A Short Note on the Kinetics-700-2020 Human Action Dataset}, 
	author={Lucas Smaira and João Carreira and Eric Noland and Ellen Clancy and Amy Wu and Andrew Zisserman},
	year={2020},
	eprint={2010.10864},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{hmdb,
	title={HMDB: a large video database for human motion recognition},
	author={Kuehne, Hildegard and Jhuang, Hueihan and Garrote, Est{\'\i}baliz and Poggio, Tomaso and Serre, Thomas},
	booktitle={2011 International conference on computer vision},
	pages={2556--2563},
	year={2011},
	organization={IEEE}
}

@misc{star-net,
	title={STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection}, 
	author={William McNally and Alexander Wong and John McPhee},
	year={2019},
	eprint={1902.10024},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@INPROCEEDINGS{dynamic-motion,
	author={Asghari-Esfeden, Sadjad and Sznaier, Mario and Camps, Octavia},
	booktitle={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
	title={Dynamic Motion Representation for Human Action Recognition}, 
	year={2020},
	volume={},
	number={},
	pages={546-555},
	doi={10.1109/WACV45572.2020.9093500}}

@misc{sipnet,
	title={Mimetics: Towards Understanding Human Actions Out of Context}, 
	author={Philippe Weinzaepfel and Grégory Rogez},
	year={2021},
	eprint={1912.07249},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@article{deeplearning,
	title={Deep learning},
	author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal={nature},
	volume={521},
	number={7553},
	pages={436--444},
	year={2015},
	publisher={Nature Publishing Group UK London}
}