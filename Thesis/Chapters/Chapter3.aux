\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{6}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{LiteratureReview}{{2}{6}{Literature Review}{chapter.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Image Classification}{6}{section.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Optical Flow}{6}{section.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A simple example of the optical flow field \textbf  {(c)}, resultant from the first and second frames \textbf  {(a)} and \textbf  {(b)}. The goal being that the background is not in the optical flow field, only the movement of subjects in the frames are considered. In this case, the only movement from frame 1 and frame 2 is a slight upward movement in the bow.\relax }}{7}{figure.caption.21}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:opticalflow}{{2.1}{7}{A simple example of the optical flow field \textbf {(c)}, resultant from the first and second frames \textbf {(a)} and \textbf {(b)}. The goal being that the background is not in the optical flow field, only the movement of subjects in the frames are considered. In this case, the only movement from frame 1 and frame 2 is a slight upward movement in the bow.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Frame 1}}}{7}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Frame 2}}}{7}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Optical Flow}}}{7}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{7}{section.25}\protected@file@percent }
\newlabel{sec:CNNs}{{2.3}{7}{Convolutional Neural Networks}{section.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Structure}{7}{subsection.26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Classical LeNet-5 architecture \blx@tocontentsinit {0}\cite {lenet5} containing convolutional, subsampling, and fully connected layers.\relax }}{8}{figure.caption.27}\protected@file@percent }
\newlabel{fig:lenet5}{{2.2}{8}{Classical LeNet-5 architecture \cite {lenet5} containing convolutional, subsampling, and fully connected layers.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of how a kernel functions, being slid across the image and multiply by the kernel matrix.\relax }}{9}{figure.caption.28}\protected@file@percent }
\newlabel{fig:kernel-example}{{2.3}{9}{Example of how a kernel functions, being slid across the image and multiply by the kernel matrix.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Classic Architectures}{9}{subsection.29}\protected@file@percent }
\newlabel{sec:classic-cnn}{{2.3.2}{9}{Classic Architectures}{subsection.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces VGG-16 \blx@tocontentsinit {0}\cite {vgg16} architecture, with 16 convolutional layers split into 5 blocks, each block being separated by pooling layers, ending with three dense layers which provide output.\relax }}{10}{figure.caption.30}\protected@file@percent }
\newlabel{fig:vgg16}{{2.4}{10}{VGG-16 \cite {vgg16} architecture, with 16 convolutional layers split into 5 blocks, each block being separated by pooling layers, ending with three dense layers which provide output.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Modern Architectures}{10}{subsection.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The Residual Block utilized by the ResNet \blx@tocontentsinit {0}\cite {resnet} model. The input is added to the output of the convolutional block, resulting in an output that has both the output of the convolutions as well as the original input, which can then be fed into further blocks.\relax }}{11}{figure.caption.32}\protected@file@percent }
\newlabel{fig:resnetresidual}{{2.5}{11}{The Residual Block utilized by the ResNet \cite {resnet} model. The input is added to the output of the convolutional block, resulting in an output that has both the output of the convolutions as well as the original input, which can then be fed into further blocks.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Recurrent Neural Networks}{11}{section.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of how an attention module will "focus" on the person performing the action, and filter out background information.\relax }}{12}{figure.caption.33}\protected@file@percent }
\newlabel{fig:residualattentionnetwork}{{2.6}{12}{Example of how an attention module will "focus" on the person performing the action, and filter out background information.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original Frame}}}{12}{figure.caption.33}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Frame Attention}}}{12}{figure.caption.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}CNN Based Action Recognition Models}{12}{section.38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The general RNN structure, similar to the fully connected networks, except it contains a recurrence value. This recurrence is stored as a 'hidden state', which can be fed back into the network at the next time step.\relax }}{13}{figure.caption.37}\protected@file@percent }
\newlabel{fig:rnn}{{2.7}{13}{The general RNN structure, similar to the fully connected networks, except it contains a recurrence value. This recurrence is stored as a 'hidden state', which can be fed back into the network at the next time step.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}CNN + LSTM Models}{13}{subsection.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }}{14}{figure.caption.40}\protected@file@percent }
\newlabel{fig:cnn-lstm}{{2.8}{14}{An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Action recognition structure for the LRCN model. \blx@tocontentsinit {0}\cite {LRCNS}\relax }}{15}{figure.caption.41}\protected@file@percent }
\newlabel{fig:lrcn-ar}{{2.9}{15}{Action recognition structure for the LRCN model. \cite {LRCNS}\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \blx@tocontentsinit {0}\cite {beyondshortsnippets}\relax }}{16}{figure.caption.42}\protected@file@percent }
\newlabel{fig:beyondoverview}{{2.10}{16}{Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \cite {beyondshortsnippets}\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}3D CNN Models}{16}{subsection.44}\protected@file@percent }
\newlabel{sec:3dCNNModels}{{2.5.2}{16}{3D CNN Models}{subsection.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Deep LSTM module used in Beyond Short Snippets: Deep Networks for Video Classification \blx@tocontentsinit {0}\cite {beyondshortsnippets}. The module consists of five separate LSTM models chained together \relax }}{17}{figure.caption.43}\protected@file@percent }
\newlabel{fig:deepLSTM}{{2.11}{17}{Deep LSTM module used in Beyond Short Snippets: Deep Networks for Video Classification \cite {beyondshortsnippets}. The module consists of five separate LSTM models chained together \relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces A portion of the original 3D-CNN action recognition model architecture proposed by \textbf  {3D Convolutional Neural Networks for Human Action Recognition} \blx@tocontentsinit {0}\cite {3DCNN-ActionRecognition}, containing a 3D convolution layer and 2x2 subsampling layer. The full architecture contains three convolutional layers, two subsampling layers and one fully connected layer.\relax }}{18}{figure.caption.45}\protected@file@percent }
\newlabel{fig:original3dcnn}{{2.12}{18}{A portion of the original 3D-CNN action recognition model architecture proposed by \textbf {3D Convolutional Neural Networks for Human Action Recognition} \cite {3DCNN-ActionRecognition}, containing a 3D convolution layer and 2x2 subsampling layer. The full architecture contains three convolutional layers, two subsampling layers and one fully connected layer.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Model Evolution}{18}{section.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The detailed inception submodule used in the I3D model \blx@tocontentsinit {0}\cite {i3d}.\relax }}{19}{figure.caption.46}\protected@file@percent }
\newlabel{fig:I3D}{{2.13}{19}{The detailed inception submodule used in the I3D model \cite {i3d}.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Datasets}{19}{section.50}\protected@file@percent }
\newlabel{sec:datasets}{{2.7}{19}{Datasets}{section.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The transformer encoder module utilized by the model proposed in \textbf  {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} \blx@tocontentsinit {0}\cite {transformer_og}.\relax }}{20}{figure.caption.48}\protected@file@percent }
\newlabel{fig:transformerActionRecognition}{{2.14}{20}{The transformer encoder module utilized by the model proposed in \textbf {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} \cite {transformer_og}.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \blx@tocontentsinit {0}\cite {transformer_og}\relax }}{21}{figure.caption.49}\protected@file@percent }
\newlabel{fig:attentionExample}{{2.15}{21}{Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \cite {transformer_og}\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Simplified representation of the JHMDB pose that will be used later in this thesis. Notably the foot joints are missing as they are not reliably labelled and are not used by the model. The indexes correspond to JHMDB joint indexes as if the subject were facing the camera.\relax }}{22}{figure.caption.51}\protected@file@percent }
\newlabel{fig:JHMDB}{{2.16}{22}{Simplified representation of the JHMDB pose that will be used later in this thesis. Notably the foot joints are missing as they are not reliably labelled and are not used by the model. The indexes correspond to JHMDB joint indexes as if the subject were facing the camera.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Pose Detection}{22}{section.59}\protected@file@percent }
\newlabel{sec:pose-detection}{{2.8}{22}{Pose Detection}{section.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Example JHMDB \blx@tocontentsinit {0}\cite {JHMDB} joint indices overlayed on a frame of the 'catch' action.\relax }}{23}{figure.caption.52}\protected@file@percent }
\newlabel{fig:JHMDB-example-pose}{{2.17}{23}{Example JHMDB \cite {JHMDB} joint indices overlayed on a frame of the 'catch' action.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Pose-Based Action Recognition}{23}{section.60}\protected@file@percent }
\newlabel{sec:pose-based}{{2.9}{23}{Pose-Based Action Recognition}{section.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Class counts for each class in the JHMDB dataset \blx@tocontentsinit {0}\cite {JHMDB}.\relax }}{24}{table.caption.53}\protected@file@percent }
\newlabel{tab:jhmdb-class-splits}{{2.1}{24}{Class counts for each class in the JHMDB dataset \cite {JHMDB}.\relax }{table.caption.53}{}}
\newlabel{fig:jhmdb-sit-example}{{2.18(a)}{25}{Subfigure 2 2.18(a)}{subfigure.55}{}}
\newlabel{sub@fig:jhmdb-sit-example}{{(a)}{25}{Subfigure 2 2.18(a)\relax }{subfigure.55}{}}
\newlabel{fig:jhmdb-golf-example}{{2.18(b)}{25}{Subfigure 2 2.18(b)}{subfigure.56}{}}
\newlabel{sub@fig:jhmdb-golf-example}{{(b)}{25}{Subfigure 2 2.18(b)\relax }{subfigure.56}{}}
\newlabel{fig:jhmdb-jump-example}{{2.18(c)}{25}{Subfigure 2 2.18(c)}{subfigure.57}{}}
\newlabel{sub@fig:jhmdb-jump-example}{{(c)}{25}{Subfigure 2 2.18(c)\relax }{subfigure.57}{}}
\newlabel{fig:jhmdb-pick-example}{{2.18(d)}{25}{Subfigure 2 2.18(d)}{subfigure.58}{}}
\newlabel{sub@fig:jhmdb-pick-example}{{(d)}{25}{Subfigure 2 2.18(d)\relax }{subfigure.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Examples from several JHMDB \blx@tocontentsinit {0}\cite {JHMDB} classes with frames sampled throughout each video.\relax }}{25}{figure.caption.54}\protected@file@percent }
\newlabel{fig:jhmdb-examples}{{2.18}{25}{Examples from several JHMDB \cite {JHMDB} classes with frames sampled throughout each video.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Sit}}}{25}{figure.caption.54}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Golf}}}{25}{figure.caption.54}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Jump}}}{25}{figure.caption.54}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Pick}}}{25}{figure.caption.54}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces Illustration of P-CNN \blx@tocontentsinit {0}\cite {PCNN} feature construction. RGB \& Optical Flow "Patches" are extracted around each joint, and sometimes containing multiple joints. These features are then passed through their respective CNN's. Note that in this figure only the RGB patches are shown.\relax }}{26}{figure.caption.61}\protected@file@percent }
\newlabel{fig:pcnn}{{2.19}{26}{Illustration of P-CNN \cite {PCNN} feature construction. RGB \& Optical Flow "Patches" are extracted around each joint, and sometimes containing multiple joints. These features are then passed through their respective CNN's. Note that in this figure only the RGB patches are shown.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.1}Intermediate Representations}{26}{subsection.64}\protected@file@percent }
\newlabel{sec:intermediate}{{2.9.1}{26}{Intermediate Representations}{subsection.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces A typical fused architecture. Each of the Pose, Optical Flow, and RGB Frames are passed through individual 3D-CNN's, the outputs are then concatenated to achieve a final output.\relax }}{27}{figure.caption.62}\protected@file@percent }
\newlabel{fig:fused}{{2.20}{27}{A typical fused architecture. Each of the Pose, Optical Flow, and RGB Frames are passed through individual 3D-CNN's, the outputs are then concatenated to achieve a final output.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces The chained architecture as shown in Chained Multi-stream Netowrks \blx@tocontentsinit {0}\cite {Chained}. The model differentiates in that it has separate loss functions for each of Pose, Optical Flow, and RGB, which are chained together in a way that they can be individually optimized.\relax }}{28}{figure.caption.63}\protected@file@percent }
\newlabel{fig:chained}{{2.21}{28}{The chained architecture as shown in Chained Multi-stream Netowrks \cite {Chained}. The model differentiates in that it has separate loss functions for each of Pose, Optical Flow, and RGB, which are chained together in a way that they can be individually optimized.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces The illustration of the PoTion intermediate representation \blx@tocontentsinit {0}\cite {potion}. The input joint heatmaps are colored based on their time in the frame, and the frames are then concatenated to form the final movement of the joint throughout the video (the figure shows only one joint heatmap, the same process is followed for all other detected joints).\relax }}{29}{figure.caption.65}\protected@file@percent }
\newlabel{fig:potion-architecture}{{2.22}{29}{The illustration of the PoTion intermediate representation \cite {potion}. The input joint heatmaps are colored based on their time in the frame, and the frames are then concatenated to form the final movement of the joint throughout the video (the figure shows only one joint heatmap, the same process is followed for all other detected joints).\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces The main PA3D \blx@tocontentsinit {0}\cite {PA3D} model architecture, demonstrating the 1x1 convolutions used in order to construct the temporal cube, in this case on the right wrist.\relax }}{30}{figure.caption.66}\protected@file@percent }
\newlabel{fig:PA3D}{{2.23}{30}{The main PA3D \cite {PA3D} model architecture, demonstrating the 1x1 convolutions used in order to construct the temporal cube, in this case on the right wrist.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces The EHPI representation used in the \textbf  {Simple yet efficient real-time pose-based action recognition} \blx@tocontentsinit {0}\cite {simple_yet_efficient}. The x, y coordinates of each joint are mapped to the red \& green values of a pixel, all joints are then stacked to form a column of joint positions in a frame. Each frame is then placed next to each other to form the 2D representation. In the case of this representation the 'B' value of RGB is always 0 since we only have x and y values.\relax }}{31}{figure.caption.67}\protected@file@percent }
\newlabel{fig:EHPI}{{2.24}{31}{The EHPI representation used in the \textbf {Simple yet efficient real-time pose-based action recognition} \cite {simple_yet_efficient}. The x, y coordinates of each joint are mapped to the red \& green values of a pixel, all joints are then stacked to form a column of joint positions in a frame. Each frame is then placed next to each other to form the 2D representation. In the case of this representation the 'B' value of RGB is always 0 since we only have x and y values.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces The representation used by the Smaller, Faster, Better model \blx@tocontentsinit {0}\cite {smaller_faster_better}, this is split into two representations. The cartesian coordinates of each joint are encoded into a 2 dimensional representation, not dissimilar to previously discussed models \blx@tocontentsinit {0}\cite {simple_yet_efficient}. The JCD feature is a similar representation, but instead of x, y, and z coordinates, uses the distance between two joints.\relax }}{31}{figure.caption.68}\protected@file@percent }
\newlabel{fig:smallerfasterbetter}{{2.25}{31}{The representation used by the Smaller, Faster, Better model \cite {smaller_faster_better}, this is split into two representations. The cartesian coordinates of each joint are encoded into a 2 dimensional representation, not dissimilar to previously discussed models \cite {simple_yet_efficient}. The JCD feature is a similar representation, but instead of x, y, and z coordinates, uses the distance between two joints.\relax }{figure.caption.68}{}}
\@setckpt{Chapters/Chapter3}{
\setcounter{page}{32}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{25}
\setcounter{table}{1}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{118}
\setcounter{maxnames}{3}
\setcounter{minnames}{3}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{38}
\setcounter{cbx@tempcntc}{0}
\setcounter{cbx@tempcntd}{-1}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
}
