\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{6}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{LiteratureReview}{{2}{6}{Literature Review}{chapter.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Image Classification}{6}{section.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Optical Flow}{6}{section.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of the optical flow field \textbf  {(c)}, resultant from the first and second frames \textbf  {(a)} and \textbf  {(b)}. The goal being that the background is not in the optical flow field, only the movement of subjects in the frames are considered. This particular example is utilizing the TV-L optical flow \blx@tocontentsinit {0}\cite {TV-L}.\relax }}{7}{figure.caption.21}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:opticalflow}{{2.1}{7}{An example of the optical flow field \textbf {(c)}, resultant from the first and second frames \textbf {(a)} and \textbf {(b)}. The goal being that the background is not in the optical flow field, only the movement of subjects in the frames are considered. This particular example is utilizing the TV-L optical flow \cite {TV-L}.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{7}{section.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Structure}{7}{subsection.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Classical LeNet-5 architecture \blx@tocontentsinit {0}\cite {lenet5} containing convolutional, subsampling, and fully connected layers.\relax }}{8}{figure.caption.24}\protected@file@percent }
\newlabel{fig:lenet5}{{2.2}{8}{Classical LeNet-5 architecture \cite {lenet5} containing convolutional, subsampling, and fully connected layers.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of kernel function as shown in Review of Deep Learning: Concepts, CNN architectures, challenges, applications, future directions \blx@tocontentsinit {0}\cite {2021cnnreview}.\relax }}{9}{figure.caption.25}\protected@file@percent }
\newlabel{fig:kernel-example}{{2.3}{9}{Example of kernel function as shown in Review of Deep Learning: Concepts, CNN architectures, challenges, applications, future directions \cite {2021cnnreview}.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Classic Architectures}{9}{subsection.26}\protected@file@percent }
\newlabel{sec:classic-cnn}{{2.3.2}{9}{Classic Architectures}{subsection.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces AlexNet \blx@tocontentsinit {0}\cite {alexnet} CNN architecture as it appears in the original paper \blx@tocontentsinit {0}\cite {alexnet}. The model is split into two separate top \& bottom parts which are fed through individual GPU's and combined at the end.\relax }}{10}{figure.caption.27}\protected@file@percent }
\newlabel{fig:alexnet}{{2.4}{10}{AlexNet \cite {alexnet} CNN architecture as it appears in the original paper \cite {alexnet}. The model is split into two separate top \& bottom parts which are fed through individual GPU's and combined at the end.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces VGG-16 \blx@tocontentsinit {0}\cite {vgg16} architecture, with 16 convolutional layers split into 5 blocks, each block being separated by pooling layers, ending with three dense layers which provide output.\relax }}{10}{figure.caption.28}\protected@file@percent }
\newlabel{fig:vgg16}{{2.5}{10}{VGG-16 \cite {vgg16} architecture, with 16 convolutional layers split into 5 blocks, each block being separated by pooling layers, ending with three dense layers which provide output.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Modern Architectures}{10}{subsection.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The Residual Block utilized by the ResNet \blx@tocontentsinit {0}\cite {resnet} model. The input is added to the output of the convolutional block, resulting in an output that has both the output of the convolutions as well as the original input, which can then be fed into further blocks.\relax }}{11}{figure.caption.30}\protected@file@percent }
\newlabel{fig:resnetresidual}{{2.6}{11}{The Residual Block utilized by the ResNet \cite {resnet} model. The input is added to the output of the convolutional block, resulting in an output that has both the output of the convolutions as well as the original input, which can then be fed into further blocks.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The model used in the \textbf  {Residual Attention Network for Image Classification} \blx@tocontentsinit {0}\cite {residualattentionnetwork}, containing Attention Modules that are built from residual units as seen in the ResNet \blx@tocontentsinit {0}\cite {resnet} model.\relax }}{12}{figure.caption.31}\protected@file@percent }
\newlabel{fig:residualattentionnetwork}{{2.7}{12}{The model used in the \textbf {Residual Attention Network for Image Classification} \cite {residualattentionnetwork}, containing Attention Modules that are built from residual units as seen in the ResNet \cite {resnet} model.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}CNN Based Models}{12}{section.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}CNN + LSTM Models}{12}{subsection.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }}{13}{figure.caption.34}\protected@file@percent }
\newlabel{fig:cnn-lstm}{{2.8}{13}{An example structure of a simple CNN-LSTM based model, each individual frame being individually fed into the CNN, and then passed to a LSTM.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Action recognition structure for the LRCN model. \blx@tocontentsinit {0}\cite {LRCNS}\relax }}{14}{figure.caption.35}\protected@file@percent }
\newlabel{fig:lrcn-ar}{{2.9}{14}{Action recognition structure for the LRCN model. \cite {LRCNS}\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \blx@tocontentsinit {0}\cite {beyondshortsnippets}\relax }}{14}{figure.caption.36}\protected@file@percent }
\newlabel{fig:beyondoverview}{{2.10}{14}{Overview of the Beyond Short Snippets: Deep Networks for Video Classification model \cite {beyondshortsnippets}\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Deep LSTM architecture utilized by \blx@tocontentsinit {0}\cite {beyondshortsnippets} in the feature aggregation step as shown in figure \ref {fig:beyondoverview}.\relax }}{15}{figure.caption.37}\protected@file@percent }
\newlabel{fig:deeplstm}{{2.11}{15}{Deep LSTM architecture utilized by \cite {beyondshortsnippets} in the feature aggregation step as shown in figure \ref {fig:beyondoverview}.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}3D CNN Models}{15}{subsection.38}\protected@file@percent }
\newlabel{sec:3dCNNModels}{{2.4.2}{15}{3D CNN Models}{subsection.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The original 3D-CNN action recognition model architecture proposed by \textbf  {3D Convolutional Neural Networks for Human Action Recognition} \blx@tocontentsinit {0}\cite {3DCNN-ActionRecognition}, containing 3 convolutional layers, two subsampling layers, and one fully connected layer\relax }}{16}{figure.caption.39}\protected@file@percent }
\newlabel{fig:original3dcnn}{{2.12}{16}{The original 3D-CNN action recognition model architecture proposed by \textbf {3D Convolutional Neural Networks for Human Action Recognition} \cite {3DCNN-ActionRecognition}, containing 3 convolutional layers, two subsampling layers, and one fully connected layer\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The model architecture used in the I3D model \blx@tocontentsinit {0}\cite {i3d}, where the Inflated Inception-V1 architecture (left) and it's detailed submodule (right) are shown.\relax }}{17}{figure.caption.40}\protected@file@percent }
\newlabel{fig:I3D}{{2.13}{17}{The model architecture used in the I3D model \cite {i3d}, where the Inflated Inception-V1 architecture (left) and it's detailed submodule (right) are shown.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Model Evolution}{17}{section.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The original transformer model proposed in \textbf  {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} \blx@tocontentsinit {0}\cite {transformer_og}, the image is split into fixed-size patches, linearly embed them, and add positional embeddings. It is then fed into a standard Transformer Encoder architecture.\relax }}{17}{figure.caption.42}\protected@file@percent }
\newlabel{fig:transformerActionRecognition}{{2.14}{17}{The original transformer model proposed in \textbf {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale} \cite {transformer_og}, the image is split into fixed-size patches, linearly embed them, and add positional embeddings. It is then fed into a standard Transformer Encoder architecture.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \blx@tocontentsinit {0}\cite {transformer_og}\relax }}{18}{figure.caption.43}\protected@file@percent }
\newlabel{fig:attentionExample}{{2.15}{18}{Example feature outputs of how a transformer utilizes attention to focus on the main subject of a video in order to greater identify actions as shown in \cite {transformer_og}\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Datasets}{18}{section.44}\protected@file@percent }
\newlabel{sec:datasets}{{2.6}{18}{Datasets}{section.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Example classes from the Kinetics dataset \blx@tocontentsinit {0}\cite {kinetics} which demonstrate the different singular person, person-person, and person-object interactions characterized in the dataset.\relax }}{19}{figure.caption.45}\protected@file@percent }
\newlabel{fig:kinetics}{{2.16}{19}{Example classes from the Kinetics dataset \cite {kinetics} which demonstrate the different singular person, person-person, and person-object interactions characterized in the dataset.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces \textbf  {Left:} JHMDB Annotated pose, each dot denoting a joint. \textbf  {Right:} Simplified representation of this pose that will be used later in this thesis. Notably the foot joints are missing as they are not reliably labelled and are not used by the model. The indexes correspond to JHMDB joint indexes as if the subject were facing the camera.\relax }}{20}{figure.caption.46}\protected@file@percent }
\newlabel{fig:JHMDB}{{2.17}{20}{\textbf {Left:} JHMDB Annotated pose, each dot denoting a joint. \textbf {Right:} Simplified representation of this pose that will be used later in this thesis. Notably the foot joints are missing as they are not reliably labelled and are not used by the model. The indexes correspond to JHMDB joint indexes as if the subject were facing the camera.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Pose Detection}{20}{section.47}\protected@file@percent }
\newlabel{sec:pose-detection}{{2.7}{20}{Pose Detection}{section.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Demonstrating the effectiveness of the OpenPose \blx@tocontentsinit {0}\cite {openpose} model. The \textbf  {top} image showing that it is capable of distinguishing individual people, the \textbf  {bottom left} showing the Part Affinity Fields corresponding to the limb connecting the right elbow and wrist. The \textbf  {bottom right} shows a zoomed in view of these Part Affinity Fields.\relax }}{21}{figure.caption.48}\protected@file@percent }
\newlabel{fig:openpose}{{2.18}{21}{Demonstrating the effectiveness of the OpenPose \cite {openpose} model. The \textbf {top} image showing that it is capable of distinguishing individual people, the \textbf {bottom left} showing the Part Affinity Fields corresponding to the limb connecting the right elbow and wrist. The \textbf {bottom right} shows a zoomed in view of these Part Affinity Fields.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Pose-Based Action Recognition}{21}{section.49}\protected@file@percent }
\newlabel{sec:pose-based}{{2.8}{21}{Pose-Based Action Recognition}{section.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces The overall framework of the action recognition model used by \textit  {An approach to pose-based action recognition} \blx@tocontentsinit {0}\cite {WangPose}. \textbf  {(a)} \& \textbf  {(b)} show the esimated poses which are then used to create the dictionary of part poses. The temporal and spacial part sets in \textbf  {(c)} are then represented in the histograms shown in \textbf  {(d)}.\relax }}{22}{figure.caption.50}\protected@file@percent }
\newlabel{fig:wangpose}{{2.19}{22}{The overall framework of the action recognition model used by \textit {An approach to pose-based action recognition} \cite {WangPose}. \textbf {(a)} \& \textbf {(b)} show the esimated poses which are then used to create the dictionary of part poses. The temporal and spacial part sets in \textbf {(c)} are then represented in the histograms shown in \textbf {(d)}.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Illustration of P-CNN \blx@tocontentsinit {0}\cite {PCNN} feature construction. RGB \& Optical Flow "Patches" are extracted around each joint, and sometimes containing multiple joints. These features are then passed through their respective CNN's, Aggregation, \& Normalization and then concatenated to form the final P-CNN feature.\relax }}{23}{figure.caption.51}\protected@file@percent }
\newlabel{fig:pcnn}{{2.20}{23}{Illustration of P-CNN \cite {PCNN} feature construction. RGB \& Optical Flow "Patches" are extracted around each joint, and sometimes containing multiple joints. These features are then passed through their respective CNN's, Aggregation, \& Normalization and then concatenated to form the final P-CNN feature.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces A typical fused architecture. Each of the Pose, Optical Flow, and RGB Frames are passed through individual 3D-CNN's, the outputs are then concatenated to achieve a final output.\relax }}{23}{figure.caption.52}\protected@file@percent }
\newlabel{fig:fused}{{2.21}{23}{A typical fused architecture. Each of the Pose, Optical Flow, and RGB Frames are passed through individual 3D-CNN's, the outputs are then concatenated to achieve a final output.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces The chained architecture as shown in Chained Multi-stream Netowrks \blx@tocontentsinit {0}\cite {Chained}. The model differentiates in that it has separate loss functions for each of Pose, Optical Flow, and RGB, which are chained together in a way that they can be individually optimized.\relax }}{24}{figure.caption.53}\protected@file@percent }
\newlabel{fig:chained}{{2.22}{24}{The chained architecture as shown in Chained Multi-stream Netowrks \cite {Chained}. The model differentiates in that it has separate loss functions for each of Pose, Optical Flow, and RGB, which are chained together in a way that they can be individually optimized.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Intermediate Representations}{24}{subsection.54}\protected@file@percent }
\newlabel{sec:intermediate}{{2.8.1}{24}{Intermediate Representations}{subsection.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces The illustration of the PoTion intermediate representation \blx@tocontentsinit {0}\cite {potion}. The input joint heatmaps are colored based on their time in the frame, and the frames are then concatenated to form the final movement of the joint throughout the video.\relax }}{25}{figure.caption.55}\protected@file@percent }
\newlabel{fig:potion-architecture}{{2.23}{25}{The illustration of the PoTion intermediate representation \cite {potion}. The input joint heatmaps are colored based on their time in the frame, and the frames are then concatenated to form the final movement of the joint throughout the video.\relax }{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces The colourization method utilized by the PoTion model \blx@tocontentsinit {0}\cite {potion}. As the frame index moves throughout the video, the colour of the joint shifts from one to another. This can be done for any amount of colours, denoted by C, the figure shows examples for C=2 and C=3, but the same logic holds for more than 3.\relax }}{26}{figure.caption.56}\protected@file@percent }
\newlabel{fig:potion-colourization}{{2.24}{26}{The colourization method utilized by the PoTion model \cite {potion}. As the frame index moves throughout the video, the colour of the joint shifts from one to another. This can be done for any amount of colours, denoted by C, the figure shows examples for C=2 and C=3, but the same logic holds for more than 3.\relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces The main PA3D \blx@tocontentsinit {0}\cite {PA3D} model architecture, demonstrating the 1x1 convolutions used in order to construct the temporal cube.\relax }}{26}{figure.caption.57}\protected@file@percent }
\newlabel{fig:PA3D}{{2.25}{26}{The main PA3D \cite {PA3D} model architecture, demonstrating the 1x1 convolutions used in order to construct the temporal cube.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces The EHPI representation used in the \textbf  {Simple yet efficient real-time pose-based action recognition} \blx@tocontentsinit {0}\cite {simple_yet_efficient}. The x, y coordinates of each joint are mapped to the red \& green values of a pixel, all joints are then stacked to form a column of joint positions in a frame. Each frame is then placed next to each other to form the 2D representation.\relax }}{27}{figure.caption.58}\protected@file@percent }
\newlabel{fig:EHPI}{{2.26}{27}{The EHPI representation used in the \textbf {Simple yet efficient real-time pose-based action recognition} \cite {simple_yet_efficient}. The x, y coordinates of each joint are mapped to the red \& green values of a pixel, all joints are then stacked to form a column of joint positions in a frame. Each frame is then placed next to each other to form the 2D representation.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces The representation used by the Smaller, Faster, Better model \blx@tocontentsinit {0}\cite {smaller_faster_better}, this is split into two representations. The cartesian coordinates of each joint are encoded into a 2 dimensional representation, not dissimilar to previously discussed models \blx@tocontentsinit {0}\cite {simple_yet_efficient}. The JCD feature is a similar representation, but instead of x, y, and z coordinates, uses the distance between two joints.\relax }}{28}{figure.caption.59}\protected@file@percent }
\newlabel{fig:smallerfasterbetter}{{2.27}{28}{The representation used by the Smaller, Faster, Better model \cite {smaller_faster_better}, this is split into two representations. The cartesian coordinates of each joint are encoded into a 2 dimensional representation, not dissimilar to previously discussed models \cite {simple_yet_efficient}. The JCD feature is a similar representation, but instead of x, y, and z coordinates, uses the distance between two joints.\relax }{figure.caption.59}{}}
\@setckpt{Chapters/Chapter3}{
\setcounter{page}{29}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{8}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{27}
\setcounter{table}{0}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{140}
\setcounter{maxnames}{3}
\setcounter{minnames}{3}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{38}
\setcounter{cbx@tempcntc}{0}
\setcounter{cbx@tempcntd}{-1}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{25}
}
